{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFIED large_scale_unlearning.ipynb from https://github.com/meghdadk/SCRUB/blob/main/large_scale_unlearning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "import numpy as np\n",
    "import random \n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import itertools\n",
    "#from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import *\n",
    "#import models\n",
    "#from logger import *\n",
    "\n",
    "from SCRUB.helper.util import adjust_learning_rate as sgda_adjust_learning_rate\n",
    "from SCRUB.distiller_zoo.KD import DistillKL\n",
    "\n",
    "from SCRUB.helper.loops import train_distill, train_vanilla, validate\n",
    "from SCRUB.helper.pretrain import init\n",
    "from SCRUB.helper.util import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activations along the path\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "    #return (p * p.log()).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=False, num_samples=4000)\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, sampler=sampler, shuffle=False)\n",
    "    prob = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(data_loader_small, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            data, target = batch\n",
    "            output = model(data)\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "    return torch.cat(prob)\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):    \n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "    \n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "    \n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])    \n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "    #clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "    clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "    clf.fit(X_r, Y_r)\n",
    "    results = clf.predict(X_f)\n",
    "    results1 = clf.predict(X_r)\n",
    "    acc = accuracy_score(results, Y_f)\n",
    "    train_ac = accuracy_score(results1, Y_r)\n",
    "    TP, FP, TN, FN = perf_measure(Y_r, results1)\n",
    "    FPR = FP/(FP+TN)\n",
    "    FNR = FN/(FN+TP)\n",
    "    \n",
    "    print (f\"TP:{TP}, FP{FP}, TN{TN}, FN{FN}\")\n",
    "    print (f\"false negative rate: {FN/(FN+TP)}\")\n",
    "    print (f\"false positive rate: {FP/(FP+TN)}\")\n",
    "    return acc, train_acc, FPR, FNR #results.mean(), results1.mean()\n",
    "    \n",
    "def plot_entropy_dist(model,retain_loader,forget_loader,test_loader_full, title, ax):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    #train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "    #indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "    #replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader,forget_loader,test_loader_full, model)\n",
    "    #np.savetxt('retain_prob.txt', X_r[Y_r==1])\n",
    "    #np.savetxt('forget_prob.txt', X_f)\n",
    "    #np.savetxt('test_prob.txt', X_r[Y_r==0])\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=plt)\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=plt)\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=plt)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.title(title,size=18)\n",
    "    plt.xlabel('Log of Entropy',size=14)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model, name):\n",
    "    prob, train_acc, FPR, FNR = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "    print(\"Attack prob: \", prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "def get_metrics(model,dataloader,criterion,device, samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "    activations=[]\n",
    "    predictions=[]\n",
    "    if use_bn:\n",
    "        model.train()\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "        for i in range(10):\n",
    "            for batch_idx, (data, target, _) in enumerate(dataloader):\n",
    "                data, target = data.to(device), target.to(device)            \n",
    "                output = model(data)\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "    for batch_idx, (data, target, _) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)            \n",
    "        if args.lossfn=='mse':\n",
    "            target=(2*target-1)\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "        output = model(data)\n",
    "        if scrub_act:\n",
    "            G = []\n",
    "            for cls in range(num_classes):\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "                G.append(grads)\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            G = torch.stack(G).pow(2)\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "        if samples_correctness:\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "            predictions.append(get_error(output,target))\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "    if samples_correctness:\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "    else:\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name): #WTF IS THIS\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics=get_metrics(model,dataloader,criterion, device, False)\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "    return _,_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "    l2_loss = 0\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "        if p.requires_grad:\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "    return l2_loss\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "    model.eval()\n",
    "    metrics = AverageMeter()    \n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            if split=='test' and scrub_act:\n",
    "                G = []\n",
    "                for cls in range(num_classes):\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "                    G.append(grads)\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "                G = torch.stack(G).pow(2)\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "            \n",
    "            if split != 'test':\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    if not quiet:\n",
    "        log_metrics(split, metrics, epoch)\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0)\n",
    "    model_init=copy.deepcopy(model)\n",
    "    for epoch in range(epochs):\n",
    "        #run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "        train_vanilla(epoch, data_loader, model, loss_fn, optimizer, args)\n",
    "\n",
    "def test(model, data_loader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_init=copy.deepcopy(model)\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "    model_scrubf.eval()\n",
    "    modelf0.eval()\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    \n",
    "    metrics = AverageMeter()    \n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "        input, target = batch\n",
    "        \n",
    "        output_sf = model_scrubf(input)\n",
    "        G_sf = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "            G_sf.append(grads)\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "        G_sf = G_sf.pow(2)\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "        G_m0 = []\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "            \n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "        \n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "        \n",
    "        loss = loss_fn(output_sf, target)\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "    \n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%run main.py --dataset cifar100 --dataroot=data/cifar-100-python --model resnet --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run main_merged.py --dataset cifar10 --model allcnn --dataroot=data/cifar-10-batches-py/ --filters 1.0 --lr 0.01 \\\n",
    "#--resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "#--weight-decay 5e-4 --batch-size 128 --epochs 26 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run main_merged.py --dataset cifar10 --model allcnn --dataroot=data/cifar-10-batches-py/ --filters 1 --lr 0.01 \\\n",
    "# --resume checkpoints/cifar100_allcnn_1_0_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_30.pt --disable-bn \\\n",
    "# --weight-decay 5e-4 --batch-size 128 --epochs 26 \\\n",
    "# --forget-class 5 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "model = models.get_model('mlp', num_classes=2, filters_percentage=1)\n",
    "model.to(device)\n",
    "dataroot = 'data/eicu/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 1\n",
    "checkpoint_name = 'eicu_mlp_1_0_forget_None_lr_0_01_bs_2048_ls_ce_seed_1_scheduler_1'\n",
    "\n",
    "\n",
    "with open(f'logs/{checkpoint_name}.pkl', 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "\n",
    "eta = 0.01\n",
    "n = 94449\n",
    "\n",
    "G = max(train_dict['grad norm over epochs'])\n",
    "batch_size = train_dict['batch_size']\n",
    "Nepochs = train_dict['selected epoch']+1\n",
    "Nbatches = math.ceil(n/batch_size)\n",
    "L = train_dict['Lipschitz']\n",
    "\n",
    "\n",
    "#eps = 10000000\n",
    "T = Nepochs * Nbatches\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "og_name = f'eicu_mlp_1_0_forget_None_lr_0_01_bs_2048_ls_ce_seed_{seed}_scheduler_1_selected'\n",
    "full_retrain_name = f'eicu_mlp_1_0_forget_940_lr_0_01_bs_2048_ls_ce_seed_{seed}_scheduler_1_loadedfrominit_{Nepochs - 0 - 1}_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forgetting these IDs: ['009-8135' '029-2185' '002-11270' '022-101536' '027-81884' '029-6088'\n",
      " '003-18285' '027-109343' '002-21943' '002-54302' '017-64734' '018-49082'\n",
      " '022-19934' '006-48721' '009-2173' '025-10338' '035-6783' '021-190306'\n",
      " '033-25315' '002-26718' '009-487' '017-86639' '033-15944' '029-23003'\n",
      " '004-9204' '030-43409' '021-211935' '002-48177' '006-200978' '011-48327'\n",
      " '016-19537' '022-158731' '035-11739' '002-44660' '025-32034' '015-46653'\n",
      " '022-152359' '021-94606' '004-64374' '021-74836' '022-47800' '017-74617'\n",
      " '007-986' '017-76322' '016-24216' '027-167200' '009-3185' '013-38740'\n",
      " '016-28038' '018-113728' '005-30409' '030-15977' '002-63997' '027-177194'\n",
      " '035-17663' '025-49078' '021-75597' '004-8513' '017-11016' '022-27974'\n",
      " '013-14708' '015-95633' '002-62839' '006-159640' '021-62678' '007-10214'\n",
      " '002-24476' '007-406' '025-1022' '017-82508' '031-20621' '002-48718'\n",
      " '033-33384' '018-84398' '013-38101' '027-36469' '002-301' '022-77313'\n",
      " '022-185192' '027-154143' '013-15171' '025-7235' '017-24626' '027-73326'\n",
      " '015-50538' '004-29808' '017-56105' '022-91562' '021-33975' '012-269'\n",
      " '010-22217' '018-53331' '006-180612' '021-187364' '030-60192' '029-23088'\n",
      " '015-81922' '007-8662' '015-99073' '033-10089' '006-212471' '021-68875'\n",
      " '029-5487' '035-14802' '003-26330' '027-154789' '035-21000' '031-13848'\n",
      " '033-5957' '018-29548' '035-2643' '015-91103' '006-201817' '010-6470'\n",
      " '029-3365' '007-7760' '035-6858' '017-38639' '009-2257' '008-54302'\n",
      " '015-93051' '012-38445' '003-35768' '032-27653' '021-19376' '027-161528'\n",
      " '003-13210' '031-22795' '007-1722' '027-117561' '011-43370' '017-9008'\n",
      " '008-38444' '012-85047' '018-93482' '022-9935' '016-13477' '017-87922'\n",
      " '030-7471' '016-22124' '035-14104' '012-71105' '002-48833' '007-470'\n",
      " '018-98456' '010-29148' '035-11604' '018-72109' '027-191351' '002-44352'\n",
      " '027-126271' '021-222168' '022-190360' '010-11090' '006-227254'\n",
      " '021-78152' '028-23232' '013-20791' '025-55820' '022-37283' '015-56303'\n",
      " '004-4437' '035-13544' '006-139307' '006-117385' '010-34086' '002-44589'\n",
      " '027-132320' '025-11617' '028-70800' '027-215654' '006-30022' '009-9319'\n",
      " '032-14757' '007-103' '007-26' '016-35725' '027-9620' '006-24580'\n",
      " '028-54792' '017-88441' '015-97879' '030-24881' '025-52416' '029-5081'\n",
      " '013-18773' '027-147907' '030-24202' '027-212670' '029-24595' '004-90807'\n",
      " '005-61409' '018-34010' '006-111644' '031-16841' '009-10951' '031-22391'\n",
      " '002-20180' '002-75684' '007-7651' '029-13674' '013-28078' '025-21024'\n",
      " '002-11836' '027-155768' '018-92266' '017-78205' '033-14172' '016-4399'\n",
      " '006-178274' '008-57006' '033-17955' '031-16403' '027-124492' '002-16757'\n",
      " '008-20116' '031-9325' '024-14425' '022-142130' '018-4428' '012-67208'\n",
      " '027-72870' '010-42057' '004-54140' '012-32996' '017-52177' '027-77184'\n",
      " '008-20926' '011-33126' '025-6155' '027-136718' '035-22431' '002-70311'\n",
      " '031-18351' '009-16738' '007-14322' '006-11885' '025-18323' '029-29888'\n",
      " '031-21191' '016-13416' '022-179182' '035-18760' '006-180138'\n",
      " '027-137914' '031-1973' '015-34875' '003-35929' '004-82061' '021-102830'\n",
      " '021-44763' '011-247' '021-97475' '025-26948' '002-44155' '022-120880'\n",
      " '010-15914' '002-57348' '002-77417' '016-22119' '018-31537' '006-107280'\n",
      " '018-27695' '027-10032' '005-54012' '018-91956' '022-119500' '011-61977'\n",
      " '004-21479' '027-187586' '008-39356' '031-6982' '018-37547' '033-2584'\n",
      " '031-20659' '018-66714' '035-14848' '006-212468' '006-117831' '005-60485'\n",
      " '035-12274' '030-31881' '011-60000' '028-52212' '006-121116' '021-98522'\n",
      " '030-1208' '006-177964' '010-42594' '010-1340' '027-169152' '006-211658'\n",
      " '025-45705' '007-3798' '021-247452' '015-15865' '017-36377' '031-12355'\n",
      " '016-33406' '022-196384' '033-10930' '030-35669' '027-187514'\n",
      " '021-156840' '011-18860' '009-2657' '005-37649' '006-70288' '030-29239'\n",
      " '010-8661' '018-17334' '006-52420' '031-7676' '011-14333' '010-27609'\n",
      " '017-4939' '027-39432' '008-24745' '022-107437' '006-91273' '025-50858'\n",
      " '016-24196' '025-22608' '015-16291' '008-6033' '017-26659' '022-113468'\n",
      " '027-85359' '031-16192' '021-58127' '021-200656' '017-47810' '006-128264'\n",
      " '022-188223' '022-3240' '027-104629' '009-3706' '006-234870' '015-53021'\n",
      " '016-31143' '027-30868' '015-30228' '002-63568' '025-48698' '004-37175'\n",
      " '015-58834' '017-63441' '006-81854' '022-57685' '016-34414' '030-7589'\n",
      " '030-74782' '030-6357' '017-47159' '004-30295' '027-210647' '017-76577'\n",
      " '022-66956' '013-5507' '006-25777' '027-139099' '007-15415' '006-165912'\n",
      " '012-57484' '006-178301' '022-133437' '027-156152' '002-53626'\n",
      " '025-11325' '018-41886' '018-68842' '012-48284' '021-77775' '009-7107'\n",
      " '030-72522' '027-22791' '029-18664' '002-31481' '006-111240' '035-363'\n",
      " '006-32297' '016-304' '022-74813' '032-31384' '021-192508' '007-10100'\n",
      " '002-66737' '002-62471' '002-23053' '031-403' '027-10160' '006-66602'\n",
      " '005-17838' '030-61112' '007-12572' '006-167947' '018-111852' '013-26403'\n",
      " '027-97722' '007-11124' '025-57318' '011-7739' '002-39621' '032-13341'\n",
      " '021-10957' '016-10476' '030-34502' '022-99671' '009-16939' '006-154696'\n",
      " '027-84745' '027-25056' '004-91217' '006-83449' '031-7505' '016-5016'\n",
      " '018-42998' '009-54' '018-92198' '002-28897' '005-88126' '016-10729'\n",
      " '028-10822' '004-66722' '028-24269' '027-208127' '018-96053' '033-37626'\n",
      " '027-181249' '027-19095' '022-96219' '002-32838' '021-8333' '009-13550'\n",
      " '002-50590' '007-7827' '017-71558' '002-77128' '028-66697' '004-30751'\n",
      " '018-85623' '009-9659' '018-2114' '006-206902' '015-96806' '033-36754'\n",
      " '016-10768' '002-19666' '017-48487' '027-152152' '030-47725' '006-249873'\n",
      " '030-3093' '022-127913' '017-26199' '006-221075' '004-65651' '017-5487'\n",
      " '033-35509' '016-29499' '011-28400' '025-23532' '018-35032' '009-7534'\n",
      " '007-8934' '033-35778' '018-87998' '002-52455' '022-98750' '004-6720'\n",
      " '027-213916' '010-5740' '031-1728' '021-246783' '031-19927' '025-46365'\n",
      " '022-32120' '011-22204' '031-4226' '033-9564' '030-64179' '017-47790'\n",
      " '004-24788' '009-2443' '015-96048' '005-7789' '004-6284' '018-5305'\n",
      " '012-52368' '016-33652' '021-54281' '002-62756' '006-224208' '018-86346'\n",
      " '006-78314' '015-9702' '027-20452' '016-24503' '027-144087' '015-46547'\n",
      " '010-1732' '007-15471' '018-61243' '004-41656' '022-159055' '016-15194'\n",
      " '004-29574' '007-2703' '002-39386' '006-173267' '004-15000' '010-25427'\n",
      " '006-230944' '033-7217' '027-190578' '012-87547' '025-45849' '004-62507'\n",
      " '027-203502' '015-74564' '029-19202' '021-176384' '025-29608' '018-39753'\n",
      " '022-92923' '012-13773' '022-70531' '009-11602' '010-39242' '022-10471'\n",
      " '006-41631' '035-2302' '008-13599' '022-74783' '010-34833' '009-7776'\n",
      " '002-63722' '027-182941' '030-33201' '017-66586' '013-37063' '011-12088'\n",
      " '005-85927' '021-213311' '027-32398' '007-13748' '029-27940' '004-53645'\n",
      " '016-10186' '011-46779' '015-17910' '032-24742' '016-21779' '015-21288'\n",
      " '021-147095' '018-122607' '027-106056' '022-16198' '027-195933'\n",
      " '010-19116' '002-58820' '018-81720' '009-12186' '027-129587' '017-51942'\n",
      " '017-100090' '027-31762' '027-62114' '002-36313' '006-55739' '016-28673'\n",
      " '031-20805' '015-109' '017-83700' '018-32982' '008-5449' '022-118807'\n",
      " '030-55712' '006-60233' '033-2119' '006-100851' '007-13009' '022-184407'\n",
      " '030-51411' '018-78687' '027-6137' '009-9595' '031-21637' '002-11937'\n",
      " '018-46943' '021-192506' '016-18372' '018-41324' '017-14919' '010-11802'\n",
      " '012-60307' '018-115312' '007-5077' '009-9888' '006-101604' '018-22285'\n",
      " '028-41486' '005-52928' '031-19615' '004-14579' '030-61381' '022-156394'\n",
      " '030-18348' '017-65062' '006-70447' '009-11122' '029-18540' '031-9344'\n",
      " '033-22502' '006-167742' '027-93442' '032-32789' '004-3610' '015-25255'\n",
      " '032-17104' '012-73676' '002-38932' '028-21806' '032-8285' '027-72351'\n",
      " '004-12923' '006-109924' '007-12516' '016-26562' '029-20828' '021-150428'\n",
      " '002-46264' '021-241026' '035-18761' '027-188621' '015-17331' '015-59289'\n",
      " '016-20158' '017-100894' '006-220461' '031-23753' '021-156607'\n",
      " '006-243217' '006-154057' '010-35437' '002-31526' '007-4709' '004-21550'\n",
      " '009-14052' '032-27284' '028-65449' '033-36998' '027-186741' '016-35913'\n",
      " '018-13363' '005-23787' '002-30811' '006-97049' '002-41110' '028-25829'\n",
      " '018-5910' '018-95859' '035-12360' '005-50632' '003-12532' '012-69310'\n",
      " '021-232484' '006-147925' '010-36166' '027-90868' '002-68919' '029-8471'\n",
      " '021-119585' '005-16202' '007-249' '017-85407' '018-76022' '022-190392'\n",
      " '015-2464' '018-35636' '022-161583' '002-58468' '013-2835' '028-62276'\n",
      " '010-26319' '030-8378' '016-13043' '027-59800' '006-226670' '006-207120'\n",
      " '021-71763' '021-119229' '017-17812' '002-76006' '006-163887' '025-45690'\n",
      " '006-126330' '004-72619' '003-7626' '018-95138' '021-198880' '003-30996'\n",
      " '027-214052' '007-16738' '018-64974' '010-17304' '030-82274' '002-71526'\n",
      " '032-23012' '007-2139' '018-109797' '035-22301' '030-39897' '009-15992'\n",
      " '016-34917' '021-197392' '021-151920' '035-6663' '021-50751' '025-4093'\n",
      " '002-16431' '022-181044' '008-48321' '006-200744' '033-39857'\n",
      " '022-176014' '004-18464' '022-7344' '018-19877' '028-33650' '029-9809'\n",
      " '022-64326' '018-102988' '016-7456' '010-19568' '021-228651' '031-4474'\n",
      " '029-18433' '027-140518' '016-22747' '035-13762' '031-15360' '005-63515'\n",
      " '022-67989' '010-16049' '013-2951' '006-59010' '005-10602' '010-29852'\n",
      " '033-42635' '009-2696' '021-144350' '005-60305' '031-24515' '010-9958'\n",
      " '018-102540' '006-178396' '003-5689' '018-60607' '004-26465' '027-120650'\n",
      " '031-14567' '011-57676' '002-37664' '002-36711' '033-13858' '015-52673'\n",
      " '035-19423' '004-53492' '002-30062' '013-22417' '007-9460' '027-83918'\n",
      " '003-58014' '021-11411' '022-150878' '004-28693' '027-117994' '010-6634'\n",
      " '017-101851' '029-22311' '015-55609' '027-206312' '015-92797'\n",
      " '006-160844' '010-10835' '027-48333' '018-55777' '031-5246' '006-18261'\n",
      " '018-122683' '002-18779' '031-1071' '027-108943' '027-213796' '027-59526'\n",
      " '022-4713' '003-27558' '017-50984' '004-18374' '009-14092' '030-529'\n",
      " '028-65519' '022-162585' '015-74107' '010-880' '030-2904' '006-198955'\n",
      " '007-16262' '016-23475' '021-18099' '033-39061' '018-111814' '028-8574'\n",
      " '027-88320' '002-7810' '006-166960' '018-106918' '010-903' '002-3153'\n",
      " '030-35320' '021-157322' '018-45803' '011-52566' '006-238794' '005-41260'\n",
      " '033-22317' '002-38318' '003-40003' '027-50131' '022-167182' '035-1772'\n",
      " '029-13780' '009-10046' '025-5959' '031-3148' '027-12418' '004-53793'\n",
      " '035-22165' '004-83924' '025-58500' '033-24410' '006-218993' '016-7378'\n",
      " '006-167609' '004-84122' '003-56763' '027-75006' '008-32195' '027-156137'\n",
      " '017-393' '033-6640' '006-236494' '010-23862' '025-64516' '028-27552'\n",
      " '007-3612' '029-12531' '004-72690' '002-65595' '010-7047' '030-59338'\n",
      " '032-5842' '027-2942' '015-15335' '002-34579' '009-7161' '002-53703'\n",
      " '021-34470' '002-30976' '005-51100' '028-1996' '006-123876' '007-8273'\n",
      " '004-20231' '016-22812' '018-115843' '016-8339' '006-203508' '002-75207'\n",
      " '015-37011' '015-65560' '008-16568' '018-76348' '016-25342' '009-857'\n",
      " '012-16865' '025-3471' '027-163747' '006-159485' '002-27665' '031-16716'\n",
      " '006-54818' '032-29891' '006-139755' '008-29092' '004-25664' '030-44138'\n",
      " '003-12079' '006-2166' '017-58138' '015-59356' '027-155867' '016-15660'\n",
      " '031-14504' '018-97221' '018-15941' '012-35404' '011-3290' '010-3398'\n",
      " '029-15229' '006-143634' '018-80449' '021-228895' '027-23503'\n",
      " '006-128656' '021-222942' '016-22458' '035-8195' '025-38428' '015-31825'\n",
      " '006-68354' '021-80793' '015-86975' '015-70990' '012-78602' '008-10564'\n",
      " '021-132447' '010-24834' '002-74444' '018-83865' '017-80144' '016-22560'\n",
      " '030-6975' '006-198565' '010-26504' '009-1025' '018-5039' '017-32762'\n",
      " '035-13816']\n",
      "Number of training samples: 93370\n",
      "Number of validation samples: 23539\n",
      "Number of test samples: 29516\n",
      "Number of train forget samples: 1079\n",
      "Number of test forget samples: 87\n",
      "Number of test retain samples: 29429\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "loaders = datasets.get_loaders_large('eicu', num_ids_forget = 940, batch_size=2048, seed=seed, root=dataroot, augment=False, ood=False, test=True)\n",
    "\n",
    "train_loader = loaders['train_loader']\n",
    "train_forget_loader = loaders['train_forget_loader']\n",
    "forget_loader = loaders['forget_loader']\n",
    "\n",
    "test_loader = loaders['test_loader']\n",
    "test_forget_loader = loaders['test_forget_loader']\n",
    "test_retain_loader = loaders['test_retain_loader']\n",
    "\n",
    "m = n - len(train_loader.dataset)\n",
    "\n",
    "#ood_dataset = copy.deepcopy(test_loader.dataset)\n",
    "#ood_dataset.indices = np.array(new_indices)\n",
    "#class_balance = sum(train_forget_loader.dataset.targets[train_forget_loader.dataset.indices])/len(train_forget_loader.dataset) #og class balance\n",
    "#balance_classes(ood_dataset, class_balance)\n",
    "#ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=256, shuffle=True) #shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_loader = train_forget_loader\n",
    "valid_loader = loaders['valid_loader'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "m_name = og_name #f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "m0_name = full_retrain_name#f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoints/' + m_name + '.pt', weights_only='True'))\n",
    "model0.load_state_dict(torch.load('checkpoints/' + m0_name + '.pt', weights_only='True'))\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "for p in model.parameters(): #what is the point of this\n",
    "    p.data0 = p.data.clone()\n",
    "for p in model0.parameters():\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "teacher = copy.deepcopy(model)\n",
    "student = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# start with an empty Namespace\n",
    "args = Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large-scale hyperparameters\n",
    "args.optim = 'sgd'\n",
    "args.gamma = 0.99\n",
    "args.alpha = 0.001\n",
    "args.beta = 0\n",
    "args.smoothing = 0.0\n",
    "args.msteps = 2\n",
    "args.clip = 0.2\n",
    "args.sstart = 10\n",
    "args.kd_T = 4\n",
    "args.distill = 'kd'\n",
    "\n",
    "args.sgda_batch_size = 128\n",
    "args.del_batch_size = 32\n",
    "args.sgda_epochs = 3\n",
    "args.sgda_epochs = 10\n",
    "args.sgda_learning_rate = 0.0005\n",
    "#args.lr_decay_epochs = [3,5,9]\n",
    "#args.lr_decay_rate = 0.1\n",
    "args.sgda_weight_decay = 5e-4\n",
    "args.sgda_momentum = 0.9\n",
    "\n",
    "\n",
    "#small-scale hyperparameters\n",
    "# args.optim = 'adam'\n",
    "# args.gamma = 1\n",
    "# args.alpha = 0.5\n",
    "# args.beta = 0\n",
    "# args.smoothing = 0.5\n",
    "# args.msteps = 3\n",
    "# args.clip = 0.2\n",
    "# args.sstart = 10\n",
    "# args.kd_T = 2\n",
    "# args.distill = 'kd'\n",
    "\n",
    "# args.sgda_epochs = 10\n",
    "# args.sgda_learning_rate = 0.0005\n",
    "# args.lr_decay_epochs = [5,8,9]\n",
    "# args.lr_decay_rate = 0.1\n",
    "# args.sgda_weight_decay = 0.1#5e-4\n",
    "# args.sgda_momentum = 0.9\n",
    "\n",
    "\n",
    "args.lossfn = 'ce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = copy.deepcopy(teacher)\n",
    "model_s = copy.deepcopy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = nn.ModuleList([])\n",
    "module_list.append(model_s)\n",
    "trainable_list = nn.ModuleList([])\n",
    "trainable_list.append(model_s)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(args.kd_T)\n",
    "criterion_kd = DistillKL(args.kd_T)\n",
    "\n",
    "\n",
    "criterion_list = nn.ModuleList([])\n",
    "criterion_list.append(criterion_cls)    # classification loss\n",
    "criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n",
    "criterion_list.append(criterion_kd)     # other knowledge distillation loss\n",
    "\n",
    "# optimizer\n",
    "if args.optim == \"sgd\":\n",
    "    optimizer = torch.optim.SGD(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"adam\": \n",
    "    optimizer = torch.optim.Adam(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          weight_decay=args.sgda_weight_decay)\n",
    "elif args.optim == \"rmsp\":\n",
    "    optimizer = torch.optim.RMSprop(trainable_list.parameters(),\n",
    "                          lr=args.sgda_learning_rate,\n",
    "                          momentum=args.sgda_momentum,\n",
    "                          weight_decay=args.sgda_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list.append(model_t)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    module_list.cuda()\n",
    "    criterion_list.cuda()\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.860 \n",
      "maximize loss: -0.0000064061\t minimize loss: 0.57\t train_acc: 68.86005401611328\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.860 \n",
      "maximize loss: -0.0000064430\t minimize loss: 0.57\t train_acc: 68.86005401611328\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.860 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 68.86005401611328\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.860 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 68.86005401611328\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.860 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 68.86005401611328\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.953 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 68.9527359008789\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.953 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 68.9527359008789\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 68.953 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 68.9527359008789\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 69.045 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 69.04541015625\n",
      "==> SCRUB unlearning ...\n",
      " * Acc@1 69.045 \n",
      "maximize loss: 0.0000000000\t minimize loss: 0.57\t train_acc: 69.04541015625\n"
     ]
    }
   ],
   "source": [
    "acc_rs = []\n",
    "acc_fs = []\n",
    "acc_ts = []\n",
    "acc_vs = []\n",
    "for epoch in range(1, args.sgda_epochs + 1):\n",
    "\n",
    "    lr = sgda_adjust_learning_rate(epoch, args, optimizer)\n",
    "\n",
    "    print(\"==> SCRUB unlearning ...\")\n",
    "\n",
    "    acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "    acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "    acc_v, acc5_v, loss_v = validate(valid_loader, model_s, criterion_cls, args, True)\n",
    "    acc_rs.append(100-acc_r.item())\n",
    "    acc_fs.append(100-acc_f.item())\n",
    "    acc_vs.append(100-acc_v.item())\n",
    "\n",
    "    maximize_loss = 0\n",
    "    if epoch <= args.msteps:\n",
    "        maximize_loss = train_distill(epoch, forget_loader, module_list, None, criterion_list, optimizer, args, \"maximize\")\n",
    "    train_acc, train_loss = train_distill(epoch, retain_loader, module_list, None, criterion_list, optimizer, args, \"minimize\")\n",
    "\n",
    "    \n",
    "    print (\"maximize loss: {:.10f}\\t minimize loss: {:.2f}\\t train_acc: {}\".format(maximize_loss, train_loss, train_acc))\n",
    "acc_r, acc5_r, loss_r = validate(retain_loader, model_s, criterion_cls, args, True)\n",
    "acc_f, acc5_f, loss_f = validate(forget_loader, model_s, criterion_cls, args, True)\n",
    "acc_v, acc5_v, loss_v = validate(valid_loader, model_s, criterion_cls, args, True)\n",
    "acc_rs.append(100-acc_r.item())\n",
    "acc_fs.append(100-acc_f.item())\n",
    "acc_vs.append(100-acc_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=467, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (l5): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.save(model_s.state_dict(), f\"other_baselines/SCRUB_{checkpoint_name}.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indices = list(range(0,len(acc_rs)))\n",
    "plt.plot(indices, acc_rs, marker='*', alpha=1, label='retain-set')\n",
    "plt.plot(indices, acc_fs, marker='o', alpha=1, label='forget-set')\n",
    "plt.plot(indices, acc_vs, marker='.', alpha=1, label='valid-set')\n",
    "plt.legend(prop={'size': 14})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.title('SCRUB retain-, valid- and forget- set error',size=18)\n",
    "plt.xlabel('epoch',size=14)\n",
    "plt.ylabel('error',size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 94449\n",
      "Number of validation samples: 23611\n",
      "Number of test samples: 29516\n",
      "tensor(0.3434)\n",
      "tensor(0.3791)\n"
     ]
    }
   ],
   "source": [
    "#OOD\n",
    "\n",
    "def balance_classes(dataset, desired_balance, seed = None): #I think this only works if desired_balance < 0.5 \n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    labels = dataset.targets[dataset.indices]\n",
    "\n",
    "    new_indices = []\n",
    "\n",
    "    large_number_indices = []\n",
    "\n",
    "    n = len(labels)\n",
    "    num_ones = sum(labels)\n",
    "    current_balance = num_ones/n\n",
    "\n",
    "    print(current_balance)\n",
    "\n",
    "    if current_balance < desired_balance:\n",
    "        m = current_balance*n/desired_balance\n",
    "        for i in range(len(dataset.indices)):\n",
    "            if labels[i] == 1:\n",
    "                new_indices.append(dataset.indices[i])\n",
    "            elif labels[i] == 0:\n",
    "                large_number_indices.append(dataset.indices[i])\n",
    "        subset_zeros = random.sample(large_number_indices, int((1 - desired_balance)*m))\n",
    "        final_indices = new_indices + list(subset_zeros)\n",
    "    else:\n",
    "        m = (1 - current_balance)*n/(1 - desired_balance)\n",
    "        for i in range(len(dataset.indices)):\n",
    "            if labels[i] == 1:\n",
    "                large_number_indices.append(dataset.indices[i])\n",
    "            elif labels[i] == 0:\n",
    "                new_indices.append(dataset.indices[i])\n",
    "        subset_ones = random.sample(large_number_indices, int(desired_balance*m))\n",
    "        final_indices = new_indices + list(subset_ones)\n",
    "\n",
    "\n",
    "    \n",
    "    dataset.indices = np.array(final_indices)       \n",
    "\n",
    "    print(sum(dataset.targets[dataset.indices])/len(dataset)) \n",
    "\n",
    "def ood_indices():\n",
    "    loaders = datasets.get_loaders_large('eicu', batch_size=256, root=dataroot, augment=False, ood=False, test=True)\n",
    "    train_loader = loaders['train_loader']\n",
    "    test_loader = loaders['test_loader']\n",
    "\n",
    "    new_indices = []\n",
    "\n",
    "    for i in test_loader.dataset.indices:\n",
    "        if test_loader.dataset.identities[i] not in train_loader.dataset.identities:\n",
    "            new_indices.append(i)\n",
    "\n",
    "    del loaders\n",
    "    return np.array(new_indices)\n",
    "\n",
    "new_indices = ood_indices()\n",
    "\n",
    "ood_dataset = copy.deepcopy(test_loader.dataset)\n",
    "ood_dataset.indices = np.array(new_indices)\n",
    "class_balance = sum(train_forget_loader.dataset.targets[train_forget_loader.dataset.indices])/len(train_forget_loader.dataset) #og class balance\n",
    "balance_classes(ood_dataset, class_balance)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=256, shuffle=True) #shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mia import *\n",
    "from umia import mia_unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5076737882811753"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model = models.get_model('mlp', num_classes=2, filters_percentage=1)\n",
    "og_model.to(device)\n",
    "\n",
    "og_pt = torch.load('checkpoints/' + og_name + '.pt', weights_only='True')\n",
    "\n",
    "og_model.load_state_dict(og_pt)\n",
    "\n",
    "acc, auc = mia_unlearning(ood_loader, train_forget_loader, og_model, modelf, device, attack_model='LR', method='l2_distance', seed=1, balance=True, crossval=True, n_samples=100)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoints/' + m_name + '.pt', weights_only='True'))\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters()):\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model, device):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc = 0\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target, _ in tqdm(train_loader):\n",
    "        data, orig_target = data.to(device), orig_target.to(device)\n",
    "        output = model(data)\n",
    "        prob = torch.nn.functional.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "    for p in model.parameters():\n",
    "        p.grad_acc /= len(train_loader)\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1079/1079 [00:11<00:00, 96.15it/s]\n"
     ]
    }
   ],
   "source": [
    "hessian(retain_loader.dataset, modelf, device)\n",
    "#hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6, num_classes=2):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3)\n",
    "    if p.size(0) == num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var\n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    if not is_base_dist:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    else:\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "    # if p.size(0) == num_classes and num_to_forget is None:\n",
    "    #     mu[class_to_forget] = 0\n",
    "    #     var[class_to_forget] = 0.0001\n",
    "    if p.size(0) == num_classes:\n",
    "        # Last layer\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        # BatchNorm\n",
    "        var *= 10\n",
    "#         var*=1\n",
    "    return mu, var\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-6\n",
    "torch.manual_seed(seed)\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "# for i, p in enumerate(modelf0.parameters()):\n",
    "#     mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "#     p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.save(modelf.state_dict(), f\"other_baselines/Fisher_{checkpoint_name}.pt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from main import run_epoch_simple\n",
    "\n",
    "def run_epoch_simple(model, data_loader, device, criterion=torch.nn.CrossEntropyLoss(), optimizer=None):\n",
    "    model.train()\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        for batch_idx, (data, target, identity) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()    \n",
    "\n",
    "\n",
    "manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "#get model\n",
    "model.load_state_dict(torch.load('checkpoints/' + m_name + '.pt', weights_only='True'))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0, weight_decay=0)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_time = 0\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    _ = run_epoch_simple(model, train_loader, device, criterion, optimizer)\n",
    "\n",
    "    t2 = time.time()\n",
    "    train_time += np.round(t2-t1,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f\"other_baselines/Finetune_{checkpoint_name}.pt\") \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
